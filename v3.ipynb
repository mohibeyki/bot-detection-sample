{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_set(file):\n",
    "    csv_file = pd.read_csv(file + '.csv')\n",
    "    with open(file + '-dump.csv', 'w') as output:\n",
    "        for line in csv_file.text.tolist():\n",
    "            output.write(str(line) + '\\n')\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text.split('\\n')\n",
    "\n",
    "\n",
    "def extract_tokens(doc):\n",
    "    total_stems = []\n",
    "    for line in doc:\n",
    "        tokens = word_tokenize(line)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "        porter = PorterStemmer()\n",
    "        stems = []\n",
    "        for t in tokens:\n",
    "            stems.append(porter.stem(t))\n",
    "        total_stems.append(stems)\n",
    "    return total_stems\n",
    "\n",
    "\n",
    "def generate_vocab(bot_tokens, gen_tokens, max_features):\n",
    "    vocab_counter = Counter()\n",
    "    vocab_counter.update([t for row in bot_tokens for t in row])\n",
    "    vocab_counter.update([t for row in gen_tokens for t in row])\n",
    "    vocab = {key: val for key, val in vocab_counter.items() if val > 1}\n",
    "    vocab_list = sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:max_features]\n",
    "    vocab = {text: i + 1 for i, (text, _) in enumerate(vocab_list)}\n",
    "    print('vocab size:', len(vocab))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_sequence(document, vocab):\n",
    "    total_seq = []\n",
    "    for tweet in document:\n",
    "        seq = []\n",
    "        for word in tweet:\n",
    "            one_hot = [0] * (len(vocab) + 1)\n",
    "            if word in vocab:\n",
    "                one_hot[vocab[word]] += 1\n",
    "            seq.append(one_hot)\n",
    "        total_seq.append(seq)\n",
    "    return total_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(bot_train_tokens, gen_train_tokens, vocab):\n",
    "    bot_train_seq = get_sequence(bot_train_tokens, vocab)\n",
    "    gen_train_seq = get_sequence(gen_train_tokens, vocab)\n",
    "    x_train = bot_train_seq + gen_train_seq\n",
    "    y_train = [0] * len(bot_train_seq) + [1] * len(gen_train_seq)\n",
    "    train_set = list(zip(x_train, y_train))\n",
    "    random.shuffle(train_set)\n",
    "    x, y = zip(*train_set)\n",
    "    return np.array(x), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(max_features):\n",
    "    bot_train_tokens = extract_tokens(load_doc('tr-small-bot.csv'))\n",
    "    gen_train_tokens = extract_tokens(load_doc('tr-small-gen.csv'))\n",
    "    bot_test_tokens = extract_tokens(load_doc('test-bot-dump.csv'))\n",
    "    gen_test_tokens = extract_tokens(load_doc('test-gen-dump.csv'))\n",
    "    vocab = generate_vocab(bot_train_tokens, gen_train_tokens, max_features)\n",
    "    x_train, y_train = get_dataset(bot_train_tokens, gen_train_tokens, vocab)\n",
    "    x_test, y_test = get_dataset(bot_test_tokens, gen_test_tokens, vocab)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, x_data, y_data, batch_size=16):\n",
    "        'Initialization'\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.x_data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        x = sequence.pad_sequences(self.x_data[index * self.batch_size: (index + 1) * self.batch_size], maxlen=maxlen)\n",
    "        return x, self.y_data[index * self.batch_size: (index + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 7075\n",
      "(8194,) train sequences\n",
      "(2000,) test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "(x_train, y_train), (x_test, y_test) = dataset_preparation(max_features)\n",
    "print(x_train.shape, 'train sequences')\n",
    "print(x_test.shape, 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(x_train, y_train)\n",
    "validation_generator = DataGenerator(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(80, 7076), dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "512/512 [==============================] - 47s 93ms/step - loss: 0.2249 - acc: 0.9224 - val_loss: 0.4392 - val_acc: 0.7890\n",
      "Epoch 2/15\n",
      "512/512 [==============================] - 44s 86ms/step - loss: 0.0689 - acc: 0.9797 - val_loss: 0.6267 - val_acc: 0.8625\n",
      "Epoch 3/15\n",
      "512/512 [==============================] - 44s 86ms/step - loss: 0.0829 - acc: 0.9773 - val_loss: 0.5034 - val_acc: 0.8240\n",
      "Epoch 4/15\n",
      "511/512 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9802"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
